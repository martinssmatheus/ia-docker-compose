# ğŸ¤– Canal IA - Assistente Offline com LLM + GPU + FastAPI

Este projeto roda uma IA local com o modelo **LLaMA 3** utilizando [Ollama](https://ollama.com/) com aceleraÃ§Ã£o por **GPU (NVIDIA)**, acessÃ­vel por uma API FastAPI. Ideal para rodar em ambiente corporativo.
## âš™ï¸ Tecnologias Utilizadas

- [FastAPI](https://fastapi.tiangolo.com/)
- [Docker Compose](https://docs.docker.com/compose/)
- [Ollama](https://ollama.com/) (LLM offline)
- Modelo: `llama3`
- GPU: Suporte via `nvidia-docker`
- Host: Ubuntu 24.04 com NVIDIA 4070

---

## ğŸ“ Estrutura do Projeto

noe/
â”œâ”€â”€ app/
â”‚ â””â”€â”€ main.py
| â””â”€â”€ ai.py
â”‚
â”œâ”€â”€ requirements.txt # DependÃªncias Python
â”œâ”€â”€ Dockerfile # API FastAPI
â”œâ”€â”€ docker-compose.yml # OrquestraÃ§Ã£o


---

## ğŸš€ Como subir o projeto

### PrÃ©-requisitos

- Docker + Docker Compose
- Driver NVIDIA + `nvidia-docker` instalado (confira com `nvidia-smi` e `docker run --rm --gpus all nvidia/cuda:12.0-base nvidia-smi`)

### Passos

# Suba os containers com build e GPU
docker compose up --build -d

# Para atualizar o projeto, rode o script deploy.sh
./deploy.sh

# Rodar dentro do container ollama
- ollama pull llama3
- ollama pull deepseek-r1
